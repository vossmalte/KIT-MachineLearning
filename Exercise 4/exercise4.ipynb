{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 4 - ML - Grundverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Package notes:**\n",
    "We will use different packages in this exersice:\n",
    "1. Scipy:\n",
    "    We will use scipy for optimizing the hinge loss. Scipy provides a numerical optimization package with various solvers such as L-BFGS-B or SLSQP. \n",
    "2. Sklearn:\n",
    "    Sklearn is a package providing different machine learning algorithms and tools. We will not use it for machine learning algorithms here but for loading the handwritten image data set, which we will use for applying probabilistic PCA.\n",
    "3. CVXPY:\n",
    "    CVXPY can be used to solve convex optimization problems such as a quadratic program. We will use the solver for optimizing for the dual problem of the SVMs.\n",
    "    \n",
    "You can install all those packages using pip (or conda or whatever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from typing import Union, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Probabilistic PCA with Expectation Maximization (7 Points)\n",
    "In this exercise we will implement probabilistic PCA as discussed in the lecture. We will apply it on a toy task and the handwritten digit data set. We will also generate our own images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some utilities for plotting. You don't need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.scatter(X[:, 0], X[:, 1], color='b')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(0, 7)\n",
    "    plt.ylim(0, 7)\n",
    "\n",
    "def draw_2d_gaussian(mu: np.ndarray, sigma: np.ndarray, plt_std: float = 2, *args, **kwargs) -> None:\n",
    "    (largest_eigval, smallest_eigval), eigvec = np.linalg.eig(sigma)\n",
    "    phi = -np.arctan2(eigvec[0, 1], eigvec[0, 0])\n",
    "\n",
    "    plt.scatter(mu[0:1], mu[1:2], marker=\"x\", *args, **kwargs)\n",
    "\n",
    "    a = plt_std * np.sqrt(largest_eigval)\n",
    "    b = plt_std * np.sqrt(smallest_eigval)\n",
    "\n",
    "    ellipse_x_r = a * np.cos(np.linspace(0, 2 * np.pi, num=200))\n",
    "    ellipse_y_r = b * np.sin(np.linspace(0, 2 * np.pi, num=200))\n",
    "\n",
    "    R = np.array([[np.cos(phi), np.sin(phi)], [-np.sin(phi), np.cos(phi)]])\n",
    "    r_ellipse = np.array([ellipse_x_r, ellipse_y_r]).T @ R\n",
    "    plt.plot(mu[0] + r_ellipse[:, 0], mu[1] + r_ellipse[:, 1], *args, **kwargs)\n",
    "\n",
    "\n",
    "def plot_ev(mu, eig_vec_1, eig_vec_2):\n",
    "    arrow_1_end = mu + eig_vec_1\n",
    "    arrow_1_x = [mu[0], arrow_1_end[0]]\n",
    "    arrow_1_y = [mu[1], arrow_1_end[1]]\n",
    "\n",
    "    arrow_2_end = mu + eig_vec_2\n",
    "    arrow_2_x = [mu[0], arrow_2_end[0]]\n",
    "    arrow_2_y = [mu[1], arrow_2_end[1]]\n",
    "\n",
    "    plt.plot(mu[0], mu[1], 'xr')\n",
    "    plt.plot((mu + eig_vec_1)[0], (mu + eig_vec_1)[1], 'xr')\n",
    "    plt.plot(arrow_1_x, arrow_1_y, 'red')\n",
    "    plt.plot(arrow_2_x, arrow_2_y, 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1) E-Step in Probabilistic PCA (3 Points)\n",
    "We will implement the E-step in this exercise. Remember the equations in the E-Step as:\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{\\mu}_{\\boldsymbol{z}|\\boldsymbol{x}_i} = (\\boldsymbol{W^TW}+\\sigma^2\\boldsymbol{I})^{-1}\\boldsymbol{W}^T(\\boldsymbol{x}_i-\\boldsymbol{\\mu}) \\\\\n",
    "     \\boldsymbol{\\Sigma_{\\boldsymbol{z}|\\boldsymbol{x}_i}}=\\sigma^2(\\boldsymbol{W^TW}+\\sigma^2\\boldsymbol{I})^{-1},\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{x}_i$ is one sample of the data, $\\boldsymbol{W}$ is the transformation matrix, $\\sigma^2$ is the variance and $\\boldsymbol{\\mu}$ is the mean of the likelihood model. <br><br>\n",
    "Please note that we need to subtract the likelihood mean from the data. This subtraction previously was missing in the slides and we uploaded a corrected version. In the video recording you will also face that it is missing. However, the formula stated here is the one you should use.<br><br>\n",
    "Implement the E-step of the EM-Algorithm for dimensionality reduction, according to the equations stated. The dimensions of the vectors/matrices are stated in the code snippet. Make sure that you have the same dimensionality\n",
    "as stated in the comments. <br>\n",
    "The hints in the comments might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-bf3e6f937ca7>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-bf3e6f937ca7>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    mu_z =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def e_step(W, mu, X, sigma_quad):\n",
    "    \"\"\"\n",
    "    Computes/Samples the Latent vectors in matrix Z given transformation matrix W and data X.\n",
    "    :param W: Transformation matrix W (shape: [DxM], where D is data dimension, M is latent Dimension)\n",
    "    :param X: Data matrix containing the data (shape: [NxD])\n",
    "    :param sigma_quad: sigma^2, the variance of the likelihood (already in quadratic form) (shape: float)\n",
    "    :return: returns mu_z, the mean of the posterior for each sample x (shape: [NxM])\n",
    "             returns z_samples, the latent variables (shape: [MxN])\n",
    "             returns var_z, the covariance of the posterior (shape: [MxM])\n",
    "    \"\"\"\n",
    "    #############################################################################################################\n",
    "    # TODO: Implement the e-step for PPCA\n",
    "    # Hint: np.linalg.solve is useful. You could also use np.linalg.inv. But np.linalg.solve is in general prefered\n",
    "    \n",
    "    # compute mean of z -> NxM\n",
    "    mu_z = \n",
    "    # compute covariance of z -> MxM\n",
    "    var_z = \n",
    "\n",
    "    # sample z for each mean (mu_z is a Matrix (NxM), containg a mean for each data x_i) \n",
    "    z_samples = \n",
    "    #############################################################################################################\n",
    "    return mu_z, z_samples, var_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2) M-Step in Probabilistic PCA (4Points)\n",
    "We will implement the E-step in this exercise. The following equations can also be looked up in the slides\n",
    "\\begin{align*}\n",
    "    \\left(\n",
    "    \\begin{array}{c}\n",
    "    \\boldsymbol{\\mu}\\\\\n",
    "    \\boldsymbol{W}\n",
    "    \\end{array}\n",
    "    \\right) = (\\boldsymbol{Z^TZ})^{-1}\\boldsymbol{Z}^T\\boldsymbol{X},\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}\n",
    "\\boldsymbol{X}=\n",
    "    \\left(\n",
    "    \\begin{array}{c}\n",
    "    \\boldsymbol{x}_1^T\\\\\n",
    "    \\vdots\\\\\n",
    "    \\boldsymbol{x}_n^T\n",
    "    \\end{array}\n",
    "    \\right),\n",
    "\\boldsymbol{Z}=\n",
    "    \\left(\n",
    "    \\begin{array}{c}\n",
    "    1, \\boldsymbol{z}_1^T\\\\\n",
    "    \\vdots\\\\\n",
    "    1, \\boldsymbol{z}_n^T\n",
    "    \\end{array}\n",
    "    \\right).\n",
    "\\end{align*}\n",
    "$\\boldsymbol{Z}$ is the matrix containing the bias and all the latent variable samples $\\boldsymbol{z}_i$ and $\\boldsymbol{X}$ is the matrix containing all data points $\\boldsymbol{x}$. \n",
    "We further need to implement the variance:\n",
    "\\begin{align*}\n",
    "    \\sigma^2 = \\frac{1}{ND}\\sum_{i=1}^{N}\\sum_{k=1}^{D}(y_{ik}- x_{ik})^2,\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{y}_i=\\boldsymbol{W}\\boldsymbol{z}_i + \\boldsymbol{\\mu}$ and N is the number of data points and D is the dimension of the data $\\boldsymbol{x}$. \n",
    "<br><br>\n",
    "Implement the M-step of the EM-Algorithm for dimensionality reduction, according to the equations stated. The dimensions of the vectors/matrices are stated in the code snippet. Make sure that you have the same dimensionality\n",
    "as stated in the comments. <br>\n",
    "The hints in the comments might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(z_samples, X):\n",
    "    \"\"\"\n",
    "    Computes the variance and the transformation matrix W given the latent vectors in z_samples and the data \n",
    "    in matrix X.\n",
    "    :param Z: The latent variable vectors stored in z_samples (shape: [NxM])\n",
    "    :param X: Data matrix containg the data (shape: [NxD])\n",
    "    :return: returns the variance sigma_quad and the transformation matrix W (shape: [DxM]) \n",
    "    \"\"\"\n",
    "    #############################################################################################################\n",
    "    # TODO: Implement the m-step for PPCA\n",
    "    # Hint: np.linalg.solve is useful. You could also use np.linalg.inv. But np.linalg.solve is in general prefered\n",
    "    \n",
    "    # create feature matrix Z\n",
    "    Z = \n",
    "    # Calculate W_tilde (Dx(M+1)) containg the mean of the likelihood and the projection matrix W\n",
    "    W_tilde = \n",
    "    mu =\n",
    "    W = \n",
    "    # Perform the predictions y in matrix Y\n",
    "    Y = \n",
    "    \n",
    "    # calculate variance sigma_quad scalar\n",
    "    sigma_quad = \n",
    "    return sigma_quad, mu, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the EM-loop, where the E-step and the M-step alternates. You do not need to implement or change the function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ppca(X: np.ndarray, n_principle_comps: int, num_iters: int = 50):\n",
    "    np.random.seed(0)\n",
    "    W = np.random.normal(size=(X.shape[1], n_principle_comps))\n",
    "    mu_X = np.mean(X, axis=0)\n",
    "    mu = mu_X.copy()\n",
    "    sigma_quad = 1\n",
    "    for i in range(num_iters):\n",
    "        mu_z, z_samples, var_z = e_step(W, mu, X, sigma_quad)\n",
    "        sigma_quad, mu, W = m_step(z_samples, X)\n",
    "    return W, z_samples, var_z, sigma_quad, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D Toy Task from Lecture Notebook\n",
    "We will first apply pPCA on the toy task, which we also had in the lecture notebook. Here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x = np.random.uniform(1,5, size=(120, 1))\n",
    "y = x + 1 + np.random.normal(0, 0.7, size=x.shape)\n",
    "\n",
    "X = np.concatenate((x, y), axis = 1)\n",
    "plot_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform the algorithm on the data. You do not need to change anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plot_data(X)\n",
    "\n",
    "W, z_samples, var_z, sigma_quad, mu = do_ppca(X, n_principle_comps=1)\n",
    "\n",
    "x_tilde = np.dot(W, z_samples.T).T + mu                         # reproject to high-dim space\n",
    "\n",
    "C = np.dot(W, W.T) + sigma_quad*np.eye(W.shape[0])      # covariance of p(x) (reconstructed)\n",
    "\n",
    "v, U = np.linalg.eig(np.cov(X.T))\n",
    "mu_X = np.mean(X, axis=0)\n",
    "plot_ev(mu_X, 2*np.sqrt(v[0])*U[:, 0], 2*np.sqrt(v[1])*U[:, 1])\n",
    "\n",
    "draw_2d_gaussian(mu_X, C)\n",
    "\n",
    "plt.plot(x_tilde[:, 0], x_tilde[:, 1], 'o', color='orange', alpha=0.2)   # reprojected data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hand-Written digits from Lecture Notebook\n",
    "Next, we apply pPCA on the handwritten digits data set. We will consider the digit 3 only. Here is how the data looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "targets = digits.target\n",
    "\n",
    "# get the images for digit 3 only\n",
    "digits_3_indx = np.where(targets == 3)[0]\n",
    "digit_3_data = digits.data[digits_3_indx]       # shape: (183, 64)  -> (8 x 8)\n",
    "digit_3_targets = digits.target[digits_3_indx]       # only needed to verify that we load digit 3\n",
    "\n",
    "mu_X_im = np.mean(digit_3_data, axis=0)\n",
    "\n",
    "#Plot the original digit 3 images\n",
    "plt.figure()\n",
    "fig, axes = plt.subplots(10, 10, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "     ax.imshow(digit_3_data[i].reshape(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's perform ppca on the data\n",
    "n_principle_comps = 10\n",
    "W_im, z_samples_im, var_z_im, sigma_quad_im, mu_im = do_ppca(digit_3_data, n_principle_comps=n_principle_comps)\n",
    "x_tilde_im = np.dot(W_im, z_samples_im.T).T + mu_im\n",
    "\n",
    "considered_im = digit_3_data[15]\n",
    "considered_im_x_tilde = x_tilde_im[15, :]\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.title('Original')\n",
    "plt.imshow(considered_im.reshape(8, 8))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Reprojection')\n",
    "plt.imshow(considered_im_x_tilde.reshape(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the reprojected data does not look same, you should definitely see the similarity to the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Image generation\n",
    "One advantage of pPCA is that we can generate random images. The generative process, as described in the lecture is implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some vectors z\n",
    "z = np.random.normal(size=(5, n_principle_comps))\n",
    "\n",
    "# Project back to D-dim space\n",
    "y = np.dot(W_im, z.T).T + mu_im\n",
    "\n",
    "# Sample noise\n",
    "eps = np.random.normal(scale=sigma_quad, size=y.shape)\n",
    "# Get image\n",
    "x = y + eps\n",
    "\n",
    "plt.figure('Sampled Image')\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "     ax.imshow(x[i].reshape(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Feature-Based Support Vector Machine (Hinge Loss) (5 Points)\n",
    "In this exercise we will train a feature-based SVM on the two moons dataset using the hinge loss. We start by loading and visualizing the data. We will use the l-bfgs-b algorithm, provided by scipy.optimize for the optimization. All you need to know about this optimizer is that it is gradient based. Otherwise you can treat it as a black-box. Yet, it's also worth a closer look if you are interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "train_data = dict(np.load(\"two_moons.npz\", allow_pickle=True))\n",
    "train_samples = train_data[\"samples\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM \n",
    "train_labels[train_labels == 0] = -1 \n",
    "\n",
    "test_data = dict(np.load(\"two_moons_test.npz\", allow_pickle=True))\n",
    "test_samples = test_data[\"samples\"]\n",
    "test_labels = test_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM \n",
    "test_labels[test_labels == 0] = -1\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Train Data\")\n",
    "plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Test Data\")\n",
    "plt.scatter(x=test_samples[test_labels == -1, 0], y=test_samples[test_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=test_samples[test_labels == 1, 0], y=test_samples[test_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Function\n",
    "From the logistic classification exercise earlier we already know that cubic features are a good choice for the two moons, so we will reuse them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_feature_fn(samples: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: Batch of 2D data vectors [x, y] [N x dim]\n",
    "    :return cubic features: [x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, 1]\n",
    "    \"\"\"\n",
    "    x = samples[..., 0]\n",
    "    y = samples[..., 1]\n",
    "    return np.stack([x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, np.ones(x.shape[0])], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1) Hinge Loss Objective (2 Points) \n",
    "\n",
    "We will implement the hinge loss objective in this exercise. Its given by \n",
    " $$\\mathcal{L}_{\\boldsymbol{X}, \\boldsymbol{y}}(w) = \\parallel \\boldsymbol{w} \\parallel^2 + C \\sum_i^N  \\max \\left( 0, 1 - y_i \\boldsymbol{w}^T \\phi(\\boldsymbol{x}_i) \\right),$$\n",
    " where $\\boldsymbol{w}$ are our model parameters, $\\phi(\\boldsymbol{x})$ are our features (here cubic) and the $y_i \\in \\lbrace{-1, 1\\rbrace}$ are the class labels. \n",
    " \n",
    " Fill in the code snippets below. This function implements the hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> float:\n",
    "    \"\"\"\n",
    "    objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape:[N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: Factor to weight the violation of margin with (C in slides)\n",
    "    :returns svm (hinge) objective (scalar)\n",
    "    \"\"\"\n",
    "    ### TODO ###############################\n",
    "    \n",
    "    ########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2) Hinge Loss Gradient (3 Points)\n",
    "Derive and implement the gradient for the hinge loss objective stated above. For all non-differentiable points in the loss courves you can use any valid subgradient. \n",
    "\n",
    "**NOTE**: The derivation is explicitly part of the grading, so state it in the solution file, not just implement it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape: [N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: Factor to weight the violation of margin with (C in slides)\n",
    "    :returns gradient of svm objective (shape: [feature_dim])\n",
    "    \"\"\"\n",
    "    ### TODO ###############################\n",
    "    \n",
    "    ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "Finally, we can tie everything together and get our maximum margin classifier. Here we are using the L-BFGS-B optimizer provided by Scipy. With $C=1000$ you should end up at a train accuracy of 1 and a test accuracy of 0.99, but feel free to play arround with $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_fn = cubic_feature_fn\n",
    "C = 1000.0\n",
    "\n",
    "# optimization\n",
    "\n",
    "train_features = feature_fn(train_samples)\n",
    "\n",
    "# For detail see: https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html\n",
    "res = opt.minimize(\n",
    "    # pass objective\n",
    "    fun=lambda w: objective_svm(w, train_features, train_labels, C),\n",
    "    # pass initial point              \n",
    "    x0=np.ones(train_features.shape[-1]),\n",
    "    # pass function to evaluate gradient (in scipy.opt \"jac\" for jacobian)\n",
    "    jac=lambda w: d_objective_svm(w, train_features, train_labels, C),\n",
    "    # specify method to use\n",
    "    method=\"l-bfgs-b\")\n",
    "\n",
    "print(res)\n",
    "w_svm = res.x\n",
    "\n",
    "# evaluation \n",
    "test_predictions = feature_fn(test_samples) @ w_svm\n",
    "train_predictions = feature_fn(train_samples) @ w_svm\n",
    "\n",
    "predicted_train_labels = np.ones(train_predictions.shape)\n",
    "predicted_train_labels[train_predictions < 0] = -1\n",
    "print(\"Train Accuracy: \", np.count_nonzero(predicted_train_labels == train_labels) / len(train_labels))\n",
    "\n",
    "predicted_test_labels = np.ones(test_predictions.shape)\n",
    "predicted_test_labels[test_predictions < 0] = -1\n",
    "print(\"Test Accuracy: \", np.count_nonzero(predicted_test_labels == test_labels) / len(test_labels))\n",
    "\n",
    "# plot train, contour, decision boundary and margins \n",
    "plt.figure()\n",
    "plt.title(\"Max Margin Solution\")\n",
    "plt_range = np.arange(-1.5, 1.5, 0.01)\n",
    "plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "flat_plt_grid = np.reshape(plt_grid, [-1, 2])\n",
    "plt_grid_shape = plt_grid.shape[:2]\n",
    "\n",
    "pred_grid = np.reshape(feature_fn(flat_plt_grid) @ w_svm, plt_grid_shape)\n",
    "\n",
    "#plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1.0, 0.0, 1.0], colors=[\"blue\", \"black\", \"orange\"])\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "             linestyles=('-',), linewidths=(2,))\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "s0 =plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "s1 =plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Kernelized Support Vector Machine (8 Points) \n",
    "\n",
    "In this exercise we will implement another SVM on the two moons dataset, this time using the kernel trick.\n",
    "\n",
    "The kernelized dual optimization problem for training an SVM is stated in the slides and can be written as a \"Quadratic Program\", i.e., an optimization of a quadratic objective under linear equality and inequality constraints - a problem of the form\n",
    "$$ min_\\boldsymbol{x} 0.5 \\boldsymbol{x}^T \\boldsymbol{Q} \\boldsymbol{x} + \\boldsymbol{q}^T \\boldsymbol{x} ~~ \\textrm{s.t.} ~~ \\boldsymbol{G}\\boldsymbol{x} \\leq \\boldsymbol{h}, ~~ \\boldsymbol{Ax} = \\boldsymbol{b}.$$ \n",
    "\n",
    "Efficient solvers for those kind of problems are well known and implemented in most programming languages. Here we use the CVXPY library.\n",
    "\n",
    "You can treat this function as a black-box here but also feel free to have a closer look. CVXPY can be used not only for quadratic programs but in general, for any convex optimization problem. The documentation can be found here https://cvxopt.org/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def solve_qp(Q: np.ndarray, q: np.ndarray,\n",
    "             G:np.ndarray, h: Union[np.ndarray, float],\n",
    "             A:np.ndarray, b: Union[np.ndarray, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    solves quadratic problem: min_x  0.5x^T Q x + q.^T x s.t. Gx <= h and Ax = b\n",
    "      in the following 'dim' refers to the dimensionality of the optimization variable x\n",
    "    :param Q: matrix of the quadratic term of the objective, (shape [dim, dim])\n",
    "    :param q: vector for the linear term of the objective, (shape [dim])\n",
    "    :param G: factor for lhs of the inequality constraint (shape [dim], or [dim, dim])\n",
    "    :param h: rhs of the inequality constraint (shape [dim], or scalar)\n",
    "    :param A: factor for lhs of the equality constraint (shape [dim], or [dim, dim])\n",
    "    :param b: rhs of the equality constraint (shape [dim], or scalar)\n",
    "    :return: optimal x (shape [dim])\n",
    "    \"\"\"\n",
    "    x = cp.Variable(q.shape[0])\n",
    "    prob = cp.Problem(cp.Minimize(0.5 * cp.quad_form(x, Q) + q.T @ x), constraints=[G @ x <= h, A @ x == b])\n",
    "    prob.solve()\n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed the problem solved by the solver above differs from the one stated in the slides. Yet the equations from the slides can be reformulated, such that the solver can be used. \n",
    "\n",
    "### Exercise 3.1 (2 Points)  \n",
    "Formulate the kernelized svm dual problem such that the solver can be used to solve it. I.e., state the quantities you need to pass for $\\boldsymbol{Q}, \\boldsymbol{q}, \\boldsymbol{G}, \\boldsymbol{h}, \\boldsymbol{A}, \\boldsymbol{b}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2 (3 Points)\n",
    "Implement the functions below so that the SVM can be trained and use for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gaussian_kernel_matrix(x: np.ndarray, sigma: float, y: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "    \"\"\" Computes Kernel matrix K(x,y) between two sets of data points x, y  for a Gaussian Kernel with bandwidth sigma.\n",
    "    If y is not given it is assumed to be equal to x, i.e. K(x,x) is computed\n",
    "    :param x: matrix containing first set of points (shape: [N, data_dim])\n",
    "    :param sigma: bandwidth of gaussian kernel\n",
    "    :param y: matrix containing second set of points (shape: [M, data_dim])\n",
    "    :return: kernel matrix K(x,y) (shape [M, N])\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        y = x\n",
    "    ### TODO ######################\n",
    "\n",
    "    ###############################\n",
    "\n",
    "\n",
    "\n",
    "def fit_svm(samples: np.ndarray, labels: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    fits an svm (with Gaussian Kernel)\n",
    "    :param samples: samples to fit the SVM to (shape: [N, data_dim])\n",
    "    :param labels: class labels corresponding to samples (shape: [N])\n",
    "    :param sigma: bandwidth of gaussian kernel\n",
    "    :return: \"alpha\" values, weight for each datapoint in the dual formulation of SVM (shape [N])\n",
    "    \"\"\"\n",
    "    ### TODO ######################\n",
    "\n",
    "    ###############################\n",
    "\n",
    "\n",
    "\n",
    "def predict_svm(samples_query: np.ndarray, samples_train: np.ndarray, labels_train: np.ndarray,\n",
    "                alphas: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    predict labels for query samples given training data and weights\n",
    "    :param samples_query: samples to query (i.e., predict labels) (shape: [N_query, data_dim])\n",
    "    :param samples_train: samples that where used to train svm (shape: [N_train, data_dim])\n",
    "    :param labels_train: labels corresponding to samples that where used to train svm (shape: [N_train])\n",
    "    :param alphas: alphas computed by training procedure (shape: [N_train])\n",
    "    :param sigma: bandwidth of gaussian kernel\n",
    "    :return: predicted labels for query points (shape: [N_query])\n",
    "    \"\"\"\n",
    "    ### TODO ######################\n",
    "\n",
    "    ###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now execute the code, train and visualize an SVM. For $\\sigma = 0.3$ you should get a train accuracy of $1.0$ and a test accuracy of $> 0.97$. You will also get two plots. The first shows all datapoints together with the decision boundary, margins and a countour plot of the svm's predictions. The second one shows again the decision boundary and margins and support vectors (the lower the value $\\alpha_i$ is, the more transparent the corresponding point in the plot is, so you will not see most points and only the \"importent ones\", i.e., the support vectors).   \n",
    "\n",
    "### Exercise 3.3 (3 Points)\n",
    "Evaluate different values of sigma in the range of $0.01$ to $1.5$. What do you observe:\n",
    "- How does the train accuracy change for different values? Why does it behave in this way?\n",
    "- How does the test accuracy change for different values?\n",
    "- How does the number of support vectors change for different values? What is the intuition behind this?\n",
    "- For large values of $\\sigma$ (roughly $\\geq1$) you will get an \"ArpackNoConvergence\" error. This essentially means that the qp-solver was not able to find a solution. Why does this happen? How can we prevent it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.3\n",
    "\n",
    "# train\n",
    "alphas = fit_svm(train_samples, train_labels, sigma)\n",
    "\n",
    "# evaluate \n",
    "train_predictions = predict_svm(train_samples, train_samples, train_labels, alphas, sigma)\n",
    "test_predictions = predict_svm(test_samples, train_samples, train_labels, alphas, sigma)\n",
    "\n",
    "predicted_train_labels = np.ones(train_predictions.shape)\n",
    "predicted_train_labels[train_predictions < 0] = -1\n",
    "print(\"Train Accuracy: \", np.count_nonzero(predicted_train_labels == train_labels) / len(train_labels))\n",
    "\n",
    "predicted_test_labels = np.ones(test_predictions.shape)\n",
    "predicted_test_labels[test_predictions < 0] = -1\n",
    "print(\"Test Accuracy: \", np.count_nonzero(predicted_test_labels == test_labels) / len(test_labels))\n",
    "\n",
    "# plot train, contour, decision boundary and margins \n",
    "plt.figure()\n",
    "plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "flat_plt_grid = np.reshape(plt_grid, [-1, 2])\n",
    "plt_grid_shape = plt_grid.shape[:2]\n",
    "\n",
    "pred_grid = np.reshape(predict_svm(flat_plt_grid, train_samples, train_labels, alphas, sigma), plt_grid_shape)\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "             linestyles=('-',), linewidths=(2,))\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "# plot margin, decision boundary and support vectors\n",
    "plt.figure()\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "             linestyles=('-',), linewidths=(2,))\n",
    "\n",
    "# squeeze alpha values into interval [0, 1] for plotting\n",
    "alphas_plt = np.clip(alphas / np.max(alphas), a_min=0.0, a_max=1.0)\n",
    "for label, color in zip([-1, 1], [\"blue\", \"orange\"]):\n",
    "    color_rgb = colors.to_rgb(color)\n",
    "    samples = train_samples[train_labels == label]\n",
    "    color_rgba = np.zeros((len(samples), 4))\n",
    "    color_rgba[:, :3] = color_rgb\n",
    "    color_rgba[:, 3] = alphas_plt[train_labels == label]\n",
    "    plt.scatter(x=samples[:, 0], y=samples[:, 1], c=color_rgba)\n",
    "\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
