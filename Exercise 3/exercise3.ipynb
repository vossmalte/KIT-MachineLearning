{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 3 - ML - Grundverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Constrained Optimization (6 Points)\n",
    "\n",
    "You are given the following Optimization problem:\n",
    "\\begin{align*}\n",
    "    \\underset{\\boldsymbol{x}}{\\textrm{ min }}& \\boldsymbol{x}^T\\boldsymbol{M}\\boldsymbol{x} + \\boldsymbol{x}^T \\boldsymbol{h} \\\\\n",
    "        s.t.          & \\boldsymbol{x}^T \\boldsymbol{b} \\geq c,\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{M}$ is a positive definit, symmetric Matrix. Note that vectors and matrices are boldsymbol, where Matrices have capital letters. <br>\n",
    "Derive the optimal solution for $\\boldsymbol{x} $ independant of the Lagrangian multiplier(s) (i.e. you have to solve for the dual). <br> <br>\n",
    "Make sure that you mark vectors and matrices as a boldsymbol and small letters and capital letters respectively. Symbols which are not marked as boldsymbols will count as scalar. <br>\n",
    "Take care of vector/matrix multiplication and derivatives. And make use of the properties of $\\boldsymbol{M}$. Don't forget to look up matrix-vector calculus in the matrix cookbook, if you don't remember the rules. <br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) k-Means (7 Points)\n",
    "Here we will implement one of the most basic appraoches to clustering - the k-Means algorithm. \n",
    "Let us start with some basic imports and implementing functionallity to visualize our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def visualize_2d_clustering(data_points: np.ndarray, assignments_one_hot: np.ndarray, centers: np.ndarray, k: int,\n",
    "                            centers_history: Optional[np.ndarray] = None, title: Optional[str] = None):\n",
    "    \"\"\"Visualizes clusters, centers and path of centers\"\"\"\n",
    "    plt.figure(figsize=(6, 6), dpi=100)\n",
    "    assignments = np.argmax(assignments_one_hot, axis=1)\n",
    "\n",
    "    for i in range(k):\n",
    "        # get next color\n",
    "        c = next(plt.gca()._get_lines.prop_cycler)['color']\n",
    "        # get cluster\n",
    "        cur_assignments = assignments == i\n",
    "        # plot clusters\n",
    "        plt.scatter(data_points[cur_assignments, 0], data_points[cur_assignments, 1], c=c, \n",
    "                    label=\"Cluster {:02d}\".format(i))\n",
    "        \n",
    "        #plot history of centers if it is given\n",
    "        if centers_history is not None:\n",
    "            plt.scatter(centers_history[:, i, 0], centers_history[:, i, 1], marker=\"x\", c=c)\n",
    "            plt.plot(centers_history[:, i, 0], centers_history[:, i, 1], c=c)\n",
    "\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], label=\"Centers\", color=\"black\", marker=\"X\")\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we going to implement the actual algorithm. As a quick reminder, K-Means works by iterating the following steps:\n",
    "\n",
    "Start with k randomly picked centers\n",
    "\n",
    "* 1.) Assign each point to the closest center\n",
    "* 2.) Addjust centers by taking the average over all points assigned to it\n",
    " \n",
    "Implementing them will be your task for this exericse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_step(data_points: np.ndarray, centers: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Assignment Step: Computes assignments to nearest cluster\n",
    "    :param data_points: Data points to cluster  (shape: [N x data_dim])\n",
    "    :param centers: current cluster centers (shape: [k, data_dim])\n",
    "    :return Assignments (as one hot) (shape: [N, k])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the assignment step of the k-Means algorithm\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "\n",
    "def adjustment_step(data_points: np.ndarray, assignments_one_hot: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adjustment Step: Adjust centers given assignment\n",
    "    :param data_points: Data points to cluster  (shape: [N x data_dim])\n",
    "    :param assignments_one_hot: assignment to adjust to (one-hot representation) (shape: [N, k])\n",
    "    :return Adjusted Centers (shape: [k, data_dim])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the adjustment step of the k-Means algorithm\n",
    "    ############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the final algorithm, as said we initialize the centers with random data points and iterate the assignmenent and adjustment step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data_points: np.ndarray, k: int, max_iter: int = 100, vis_interval: int = 3) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simple K Means Implementation\n",
    "    :param data_points: Data points to cluster  (shape: [N x data_dim])\n",
    "    :param k: number of clusters\n",
    "    :param max_iter: Maximum number of iterations to run if convergence is not reached\n",
    "    :param vis_interval: After how many iterations to generate the next plot\n",
    "    :return: - cluster labels (shape: [N])\n",
    "             - means of clusters (shape: [k, data_dim])\n",
    "             - SSD over time (shape: [2 * num_iters])\n",
    "             - History of means over iterations (shape: [num_iters, k, data_dim])\n",
    "    \"\"\"\n",
    "    # Bookkeeping\n",
    "    i = 0\n",
    "    means_history = []\n",
    "    ssd_history = []\n",
    "    assignments_one_hot = np.zeros(shape=[data_points.shape[0], k])\n",
    "    old_assignments = np.ones(shape=[data_points.shape[0], k])\n",
    "\n",
    "    # Initialize with k random data points\n",
    "    initial_idx = np.random.choice(len(data_points), k, replace=False)\n",
    "    centers = data_points[initial_idx]\n",
    "    means_history.append(centers.copy())\n",
    "\n",
    "    # Iterate while not converged and max number iterations not reached\n",
    "    while np.any(old_assignments != assignments_one_hot) and i < max_iter:\n",
    "        old_assignments = assignments_one_hot\n",
    "        \n",
    "        # assignment \n",
    "        assignments_one_hot = assignment_step(data_points, centers)\n",
    "        \n",
    "        # compute SSD\n",
    "        diffs = np.sum(np.square(data_points[:, None, :] - centers[None, :, :]), axis=-1)\n",
    "        ssd_history.append(np.sum(assignments_one_hot * diffs))\n",
    "    \n",
    "        # adjustment\n",
    "        centers = adjustment_step(data_points, assignments_one_hot)\n",
    "        \n",
    "        # compute SSD\n",
    "        diffs = np.sum(np.square(data_points[:, None, :] - centers[None, :, :]), axis=-1)\n",
    "        ssd_history.append(np.sum(assignments_one_hot * diffs))\n",
    "        \n",
    "                \n",
    "        # Plotting\n",
    "        if i % vis_interval == 0:\n",
    "            visualize_2d_clustering(data_points, assignments_one_hot, centers, k, title=\"Iteration {:02d}\".format(i))\n",
    "\n",
    "        # Bookkeeping \n",
    "        means_history.append(centers.copy())\n",
    "        i += 1\n",
    "\n",
    "    print(\"Took\", i, \"iterations to converge\")\n",
    "    return assignments_one_hot, centers, np.array(ssd_history), np.stack(means_history, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run the dataset and visualize the results. Here we provide 4 random datasets, each containing 500 2 samples and you can play around with the number of clustes, $k$, as well as the seed of the random number generator. Based on this seed the initial centers, and thus the final outcome, will vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d162e11733de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssd_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# plot final clustering with history of centers over iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-06cb3b1c5eb0>\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(data_points, k, max_iter, vis_interval)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# compute SSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mssd_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignments_one_hot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# adjustment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = np.load(\"samples_3.npy\")\n",
    "k = 8\n",
    "\n",
    "cluster_labels, centers, ssd_history, centers_history = k_means(data, k)\n",
    "\n",
    "# plot final clustering with history of centers over iterations\n",
    "visualize_2d_clustering(data, cluster_labels, centers, k=k, centers_history=centers_history, title=\"Final Clustering\")\n",
    "\n",
    "# plot SSD\n",
    "plt.figure(\"SSD\")\n",
    "plt.semilogy(np.arange(start=0, stop=len(ssd_history) / 2, step=0.5), ssd_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"SSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Expectation Maximization for Gaussian Mixture Models (7 Points)\n",
    "\n",
    "In the following we implement the Expectation Maximization (EM) Algorithm to fit a Gaussian Mixture Model (GMM) to data. We start with an implemenation for the log density of a single Gaussian (take some time to compare this implementation with the one used in the first exercies)... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "def gaussian_log_density(samples: np.ndarray, mean: np.ndarray, covariance: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes Log Density of samples under a Gaussian Distribution.\n",
    "    We already saw an implementation of this in the first exercise and noted there that this was not the \"proper\"\n",
    "    way of doing it. Compare the two implementations.\n",
    "    :param samples: samples to evaluate (shape: [N x dim)\n",
    "    :param mean: Mean of the distribution (shape: [dim])\n",
    "    :param covariance: Covariance of the distribution (shape: [dim x dim])\n",
    "    :return: log N(x|mean, covariance) (shape: [N])\n",
    "    \"\"\"\n",
    "    dim = mean.shape[0]\n",
    "    chol_covariance = np.linalg.cholesky(covariance)\n",
    "    # Efficient and stable way to compute the log determinant and squared term efficiently using the cholesky\n",
    "    logdet = 2 * np.sum(np.log(np.diagonal(chol_covariance) + 1e-25))\n",
    "    # (Actually, you would use scipy.linalg.solve_triangular but I wanted to spare you the hustle of setting\n",
    "    #  up scipy)\n",
    "    chol_inv = np.linalg.inv(chol_covariance)\n",
    "    exp_term = np.sum(np.square((samples - mean) @ chol_inv.T), axis=-1)\n",
    "    return -0.5 * (dim * np.log(2 * np.pi) + logdet + exp_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and some plotting functionaliy for 2D GMMs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_2d_gmm(samples, weights, means, covs, title):\n",
    "    \"\"\"Visualizes the model and the samples\"\"\"\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.title(title)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], label=\"Samples\", c=next(plt.gca()._get_lines.prop_cycler)['color'])\n",
    "\n",
    "    for i in range(means.shape[0]):\n",
    "        c = next(plt.gca()._get_lines.prop_cycler)['color']\n",
    "\n",
    "        (largest_eigval, smallest_eigval), eigvec = np.linalg.eig(covs[i])\n",
    "        phi = -np.arctan2(eigvec[0, 1], eigvec[0, 0])\n",
    "\n",
    "        plt.scatter(means[i, 0:1], means[i, 1:2], marker=\"x\", c=c)\n",
    "\n",
    "        a = 2.0 * np.sqrt(largest_eigval)\n",
    "        b = 2.0 * np.sqrt(smallest_eigval)\n",
    "\n",
    "        ellipse_x_r = a * np.cos(np.linspace(0, 2 * np.pi, num=200))\n",
    "        ellipse_y_r = b * np.sin(np.linspace(0, 2 * np.pi, num=200))\n",
    "\n",
    "        R = np.array([[np.cos(phi), np.sin(phi)], [-np.sin(phi), np.cos(phi)]])\n",
    "        r_ellipse = np.array([ellipse_x_r, ellipse_y_r]).T @ R\n",
    "        plt.plot(means[i, 0] + r_ellipse[:, 0], means[i, 1] + r_ellipse[:, 1], c=c,\n",
    "                 label=\"Component {:02d}, Weight: {:0.4f}\".format(i, weights[i]))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the actual task: You need to implement 3 functions:\n",
    "- the log likelihhod of a GMM for evaluation\n",
    "- the E-Step of the EM algorithm for GMMs\n",
    "- the M-Step of the EM algorithm for GMMs (for this one now for loops are allowed. Using them here will lead to point deduction)\n",
    "\n",
    "\n",
    "All needed equations are in the slides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gmm_log_likelihood(samples: np.ndarray, weights: np.ndarray, means: np.ndarray, covariances: np.ndarray) -> float:\n",
    "    \"\"\" Computes the Log Likelihood of samples given parameters of a GMM.\n",
    "    :param samples: samples \"x\" to compute ess for    (shape: [N, dim])\n",
    "    :param weights: weights (i.e., p(z) ) of old model (shape: [num_components])\n",
    "    :param means: means of old components p(x|z) (shape: [num_components, dim])\n",
    "    :param covariances: covariances of old components p(x|z) (shape: [num_components, dim, dim]\n",
    "    :return: log likelihood\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the log-likelihood for Gaussian Mixtures\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "def e_step(samples: np.ndarray, weights: np.ndarray, means: np.ndarray, covariances: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" E-Step of EM for fitting GMMs. Computes estimated sufficient statistics (ess), p(z|x), using the old model from\n",
    "    the previous iteration. In the GMM case they are often referred to as \"responsibilities\".\n",
    "    :param samples: samples \"x\" to compute ess for    (shape: [N, dim])\n",
    "    :param weights: weights (i.e., p(z) ) of old model (shape: [num_components])\n",
    "    :param means: means of old components p(x|z) (shape: [num_components, dim])\n",
    "    :param covariances: covariances of old components p(x|z) (shape: [num_components, dim, dim]\n",
    "    :return: Responsibilities p(z|x) (Shape: [N x num_components])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the E-Step for EM for Gaussian Mixtrue Models.\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "def m_step(samples: np.ndarray, responsibilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" M-Step of EM for fitting GMMs. Computes new parameters given samples and responsibilities p(z|x)\n",
    "    :param samples: samples \"x\" to fit model to (shape: [N, dim])\n",
    "    :param responsibilities: p(z|x) (Shape: [N x num_components]), as computed by E-step\n",
    "    :return: - new weights p(z) (shape [num_components])\n",
    "             - new means of components p(x|z) (shape: [num_components, dim])\n",
    "             - new covariances of components p(x|z) (shape: [num_components, dim, dim]\n",
    "    \"\"\"\n",
    "    #########################################################\n",
    "    # TODO: Implement the M-Step for EM for Gaussian Mixture models. You are not allowed to use any for loops!\n",
    "    # Hint: Writing it directly without for loops is hard, especially if you are not experienced with broadcasting.\n",
    "    # It's maybe easier to first implement it using for loops and then try getting rid of them, one after another.\n",
    "    #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap out functions with the actual algorithm, iterating E and M step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fit_gaussian_mixture(samples: np.ndarray, num_components: int, num_iters: int = 30, vis_interval: int = 5):\n",
    "    \"\"\"Fits a Gaussian Mixture Model using the Expectation Maximization Algorithm\n",
    "    :param samples: Samples to fit the model to (shape: [N, dim]\n",
    "    :param num_components: number of components of the GMM\n",
    "    :param num_iters: number of iterations\n",
    "    :param vis_interval: After how many iterations to generate the next plot\n",
    "    :return: - final weights p(z) (shape [num_components])\n",
    "             - final means of components p(x|z) (shape: [num_components, dim])\n",
    "             - final covariances of components p(x|z) (shape: [num_components, dim, dim]\n",
    "             - log_likelihoods: log-likelihood of data under model after each iteration (shape: [num_iters])\n",
    "    \"\"\"\n",
    "    # Initialize Model: We initialize with means randomly picked from the data, unit covariances and uniform\n",
    "    # component weights. This works here but in general smarter initialization techniques might be necessary, e.g.,\n",
    "    # k-means\n",
    "    initial_idx = np.random.choice(len(samples), num_components, replace=False)\n",
    "    means = samples[initial_idx]\n",
    "    covs = np.tile(np.eye(data.shape[-1])[None, ...], [num_components, 1, 1])\n",
    "    weights = np.ones(num_components) / num_components\n",
    "\n",
    "    # bookkeeping:\n",
    "    log_likelihoods = np.zeros(num_iters)\n",
    "\n",
    "    # iterate E and M Steps\n",
    "    for i in range(num_iters):\n",
    "        responsibilities = e_step(samples, weights, means, covs)\n",
    "        weights, means, covs = m_step(samples, responsibilities)\n",
    "\n",
    "        # Plotting\n",
    "        if i % vis_interval == 0:\n",
    "            visualize_2d_gmm(data, weights, means, covs, title=\"After Iteration {:02d}\".format(i))\n",
    "\n",
    "        log_likelihoods[i] = gmm_log_likelihood(samples, weights, means, covs)\n",
    "    return weights, means, covs, log_likelihoods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load some data and run the algorithm. Feel free to play around with the parameters a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b00577163f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# running and ploting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfinal_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_covariances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likeihoods\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mfit_gaussian_mixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvisualize_2d_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_covariances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Final Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-302f370d3500>\u001b[0m in \u001b[0;36mfit_gaussian_mixture\u001b[0;34m(samples, num_components, num_iters, vis_interval)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mresponsibilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponsibilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "## ADAPTABLE PARAMETERS: \n",
    "\n",
    "np.random.seed(0)\n",
    "num_components = 5\n",
    "num_iters = 30\n",
    "vis_interval = 5\n",
    "\n",
    "# CHOOSE A DATASET\n",
    "#data = np.load(\"samples_1.npy\")\n",
    "data = np.load(\"samples_2.npy\")\n",
    "#data = np.load(\"samples_3.npy\")\n",
    "#data = np.load(\"samples_u.npy\")\n",
    "\n",
    "# running and ploting\n",
    "final_weights, final_means, final_covariances, log_likeihoods = \\\n",
    "    fit_gaussian_mixture(data, num_components, num_iters, vis_interval)\n",
    "visualize_2d_gmm(data, final_weights, final_means, final_covariances, title=\"Final Model\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Log-Likelihoods over time\")\n",
    "plt.plot(log_likeihoods)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
