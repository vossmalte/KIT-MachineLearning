{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Neural Network Classifier from Scratch (10p.)\n",
    "\n",
    "In this exercise we will implement a small neural network from scratch, i.e., only using numpy. This is nothing you would do \"in real life\" but it is a good exercise to deepen understanding. \n",
    "\n",
    "The network will consist of an arbitrary number of hidden layers with ReLU activation, a sigmoid output layer (as we are doing binary classification) and we will train it using the binary cross entropy (negative bernoulli likelihood). Ok, so lets start by importing and loading what we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load our two moons (I promise we will get a new dataset in the next exercise)\n",
    "train_data = dict(np.load(\"two_moons.npz\", allow_pickle=True)) \n",
    "test_data = dict(np.load(\"two_moons_test.npz\", allow_pickle=True))\n",
    "# we need to reshape our labels so that they are [N, 1] and not [N] anymore\n",
    "train_samples, train_labels = train_data[\"samples\"], train_data[\"labels\"][:, None]\n",
    "test_samples, test_labels = test_data[\"samples\"], test_data[\"labels\"][:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1.) Auxillary Functions (3 p.)\n",
    "We start with implementing some auxillary functions we are going to need later. The sigmoid and relu activation functions, the binary cross entropy loss as well as their derviatives. \n",
    "\n",
    "The binary cross entropy loss is given as \n",
    "$ - \\dfrac{1}{N} \\sum_{i=1}^N (y_i \\log (p_i) + (1 - y_i) \\log (1 - p_i)) $ where $y_i$ denotes the ground truth label and $p_i$ the network prediction for sample $i$.\n",
    "\n",
    "**Hint** all derivatives where derived/implemented during the lecture or previous exercise - so feel free to borrow them from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise relu activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : relu(x) [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    x[x <= 0] = 0\n",
    "    return x\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise gradient of relu activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : d relu(x) / dx [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    x[x <= 0] = 0\n",
    "    x[x != 0] = 1\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise sigmoid activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : d sigmoid(x) /dx [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    #return np.exp(x) / (1 + (np.exp(x)))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise sigmoid activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : sigmoid(x) [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def binary_cross_entropy(predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    binary cross entropy loss (negative bernoulli ll)\n",
    "\n",
    "    The binary cross entropy loss is given as \n",
    "    $ - \\dfrac{1}{N} \\sum_{i=1}^N (y_i \\log (p_i) + (1 - y_i) \\log (1 - p_i)) $ where $y_i$ denotes the ground truth label     and $p_i$ the network prediction for sample $i$.\n",
    "\n",
    "    :param predictions: predictions by model (shape [N])\n",
    "    :param labels: class labels corresponding to train samples, (shape: [N])\n",
    "    :return binary cross entropy\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    return -np.sum(labels * np.log(predictions) + (1 - labels) * np.log(1 - predictions)) / predictions.shape[0]\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_binary_cross_entropy(predictions: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of the binary cross entropy loss\n",
    "    :param predictions: predictions by model (shape [N])\n",
    "    :param labels: class labels corresponding to train samples, (shape [N])\n",
    "    :return gradient of binary cross entropy, w.r.t. the predictions (shape [N])\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    return ( labels / predictions - (1 - labels) / (1 - predictions) ) * (-1)\n",
    "    \n",
    "    ##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup & Intialization\n",
    "\n",
    "Next we are going to set up the Neural Network. We will represent it as a list of weight matrices and a list of bias vectors. Each list has one entry for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(neurons_per_hidden_layer: List[int], input_dim: int, output_dim: int, seed: int = 0) \\\n",
    "        -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    :param neurons_per_hidden_layer: list of numbers, indicating the number of neurons of each hidden layer\n",
    "    :param input_dim: input dimension of the network\n",
    "    :param output_dim: output dimension of the network\n",
    "    :param seed: seed for random number generator\n",
    "    :return list of weights and biases as specified by dimensions and hidden layer specification\n",
    "    \"\"\"\n",
    "    # seed random number generator\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scale_factor = 1.0\n",
    "    prev_n = input_dim\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    # hidden layers\n",
    "    for n in neurons_per_hidden_layer:\n",
    "        # initialize weights with gaussian noise\n",
    "        weights.append(scale_factor * rng.normal(size=[prev_n, n]))\n",
    "        # initialize bias with zeros\n",
    "        biases.append(np.zeros([1, n]))\n",
    "        prev_n = n\n",
    "\n",
    "    # output layer\n",
    "    weights.append(scale_factor * rng.normal(size=[prev_n, output_dim]))\n",
    "    biases.append(np.zeros([1, output_dim]))\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** As NNs are non-convex, initialization plays a very important role in NN training and there is a lot of work into how to initialize them properly - this here is not a very good initialization, but sufficient for our small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Forward Pass (3 p.)\n",
    "\n",
    "Next step is the forward pass, i.e., propagate a batch of samples through the network to get the final prediciton.\n",
    "But that's not all - to compute the gradietns later we also need to store all necessary quantities, here those are:\n",
    "- The input to every layer (here called h's)\n",
    "- The \"pre-activation\" of every layer, i.e., the qantity that is fed into the non-linearity (here called z's)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x: np.ndarray, weights: List[np.ndarray], biases: List[np.ndarray])\\\n",
    "        -> Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    propagate input through network\n",
    "    :param x: input: (shape, [N x input_dim])\n",
    "    :param weights: weight parameters of the layers\n",
    "    :param biases: bias parameters of the layers\n",
    "    :return: - Predictions of the network (shape, [N x out_put_dim])\n",
    "             - hs: output of each layer (input + all hidden layers) (length: len(weights))\n",
    "             - zs: preactivation of each layer (all hidden layers + output) (length: len(weights))\n",
    "    \"\"\"\n",
    "\n",
    "    hs = []  # list to store all inputs\n",
    "    zs = []  # list to store all pre-activations\n",
    "    \n",
    "    # input to first hidden layer is just the input to the network \n",
    "    h = x\n",
    "    # hs.append(h)\n",
    "    \n",
    "    ### TODO #########################\n",
    "    # pass \"h\" to all hidden layers\n",
    "    # record all inputs and pre-activations in the lists\n",
    "    ##################################\n",
    "    #print(\"FW\")\n",
    "    for n in range(len(weights)):\n",
    "        hs.append(h)\n",
    "        #print(\"\\t\",weights[n].shape)\n",
    "        #print(weights[n].shape, \"@\", h.shape)\n",
    "        z = h @ weights[n] + biases[n]\n",
    "        zs.append(z)\n",
    "        h = sigmoid(z)\n",
    "    y = h\n",
    "    #y = sigmoid(z)   # z denotes the pre-activation of the output layer here. Feel free to rename it\n",
    "\n",
    "    return y, hs, zs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Backward Pass (4 p.)\n",
    "\n",
    "For training by gradient descent we need - well - gradients. Those are computed using backpropagation during the so called \"backward pass\". We will use the chain rule to propagate the gradient back through the network and at every layer, compute the gradients for the weights and biases at that layer. The initial gradient is given by the gradient of the loss function w.r.t. the network output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(loss_grad: np.ndarray, \n",
    "                  hs: List[np.ndarray], zs: List[np.ndarray], \n",
    "                  weights: List[np.ndarray], biases: List[np.ndarray]) -> \\\n",
    "    Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    propagate gradient backwards through network\n",
    "    :param loss_grad: gradient of the loss function w.r.t. the network output (shape: [N, 1])\n",
    "    :param hs: values of all hidden layers during forward pass\n",
    "    :param zs: values of all preactivations during forward pass\n",
    "    :param weights: weight paramameters of the layers\n",
    "    :param biases: bias parameters of the layers\n",
    "    :return: d_weights: List of weight gradients - one entry with same shape for each entry of \"weights\"\n",
    "             d_biases: List of bias gradients - one entry with same shape for each entry of \"biases\"\n",
    "    \"\"\"\n",
    "\n",
    "    # return gradients as lists - we pre-initialize the lists as we iterate backwards\n",
    "    d_weights = [None] * len(weights)\n",
    "    d_biases = [None] * len(biases)\n",
    "\n",
    "    # sketch\n",
    "    # y = sigmoid(Wx + b)\n",
    "    # h0 ---wx+b> z0 ---sigm> ... ---sigm> h_n ---wx+b> z_n ---sigm> y ---cross_entr> L\n",
    "    # loss_grad = dL / dy = y_bar\n",
    "\n",
    "    # next step:\n",
    "    # calculate z_n_bar = dL / dz_n = loss_grad * d_sigmoid(z_n)\n",
    "    \n",
    "    # calculate w_n_bar = dL / dw_n = z_n_bar outer h_n\n",
    "    # calculate b_n_bar = z_n_bar\n",
    "    # calculate h_n_bar = dL / dh_n = w_n.T @ z_n_bar\n",
    "\n",
    "    ### TODO #########################\n",
    "    #print(\"BP\")\n",
    "    prev_out_grad = loss_grad\n",
    "    for n in reversed(range(len(weights))):\n",
    "        #print(\"prev_bar\",prev_out_grad.shape,\"z\",d_sigmoid(zs[n]).shape)\n",
    "        z_n_bar = prev_out_grad * d_sigmoid(zs[n])\n",
    "        d_weights[n] = hs[n].T @ z_n_bar / len(loss_grad)\n",
    "        #print(\"\\t\", d_weights[n].shape)\n",
    "        d_biases[n]  = z_n_bar.sum(axis=0)\n",
    "        #print(weights[n].shape, \"@\", z_n_bar.shape)\n",
    "        prev_out_grad = z_n_bar @ weights[n].T\n",
    "        #print(prev_out_grad.shape)\n",
    "\n",
    "        \n",
    "\n",
    "    ##################################\n",
    "\n",
    "    return d_weights, d_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying Everything Together \n",
    "\n",
    "Finally we can tie everything together and train our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyper parameters \n",
    "layers = [64, 64]\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# init model\n",
    "weights, biases = init_weights(layers, input_dim=2, output_dim=1, seed=42)\n",
    "\n",
    "\n",
    "#book keeping\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Here we work with a simple gradient descent implementation, using the whole dataset at each iteration,\n",
    "# You can modify it to stochastic gradient descent or a batch gradient descent procedure as an exercise\n",
    "for i in range(1000):\n",
    "    \n",
    "    # predict network outputs and record intermediate quantities using the forward pass\n",
    "    prediction, hs, zs = forward_pass(train_samples, weights, biases)\n",
    "    train_losses.append(binary_cross_entropy(prediction, train_labels))\n",
    "\n",
    "    # compute gradients\n",
    "    loss_grad = d_binary_cross_entropy(prediction, train_labels)\n",
    "    w_grads, b_grads = backward_pass(loss_grad, hs, zs, weights, biases)\n",
    "\n",
    "    # apply gradients\n",
    "    for i in range(len(w_grads)):\n",
    "        weights[i] -= learning_rate * w_grads[i]\n",
    "        biases[i] -= learning_rate * b_grads[i]\n",
    "\n",
    "    test_losses.append(binary_cross_entropy(forward_pass(test_samples, weights, biases)[0], test_labels))\n",
    "\n",
    "# plotting\n",
    "plt.title(\"Loss\")\n",
    "plt.semilogy(train_losses)\n",
    "plt.semilogy(test_losses)\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "\n",
    "def plt_solution(samples, labels):\n",
    "    plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "    plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "    plt_grid_shape = plt_grid.shape[:2]\n",
    "    pred_grid = np.reshape(forward_pass(plt_grid, weights, biases)[0], plt_grid_shape)\n",
    "    plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[0.5], colors=[\"black\"])\n",
    "    plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "    plt.colorbar()\n",
    "    s0 = plt.scatter(x=samples[labels[:, 0] == 0, 0], y=samples[labels[:, 0] == 0, 1],\n",
    "                     label=\"c=0\", c=\"blue\")\n",
    "    s1 = plt.scatter(x=samples[labels[:, 0] == 1, 0], y=samples[labels[:, 0] == 1, 1],\n",
    "                     label=\"c=1\", c=\"orange\")\n",
    "    plt.legend([s0, s1], [\"c0\", \"c1\"])\n",
    "    plt.xlim(-1.5, 2.5)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Trained Network - with train samples\")\n",
    "plt_solution(train_samples, train_labels)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Trained Network - with test samples\")\n",
    "plt_solution(test_samples, test_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) MNIST Classifier with PyTorch (10 p.)\n",
    "\n",
    "Modern deep learning approaches are mostly implemented using special libraries, providing functionality such as automatic differentiation, common SGD Optimiziers, easy usage of GPUs and so on. We will use PyTorch, at the moment the, arguably, most common framework (for research).\n",
    "\n",
    "## Getting Started\n",
    "You can find a documentation of the PyTorch API here https://pytorch.org/docs/stable/torch.html# . Don't worry if it seems a lot, we will point out the relevant bits during the exercise as we go along \n",
    "\n",
    "**Installation** \n",
    "You can find installation instructions here https://pytorch.org/ . Take the most recent stable version (1.7.X). We won't use GPUs here so you can take the cuda-free installation. We also don't need torchvision nor torchaudio so those don't need to be installed.\n",
    "\n",
    "**Data**\n",
    "We finally use a new dataset. The classical MNIST Handwritten Digit Classification set. It consists of grayscale images of size 28x28 of handwritten digits. Let's load it and visualize some of the images. We also do some preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data_dict = dict(np.load(\"mnist.npz\"))\n",
    "\n",
    "# prepare data:\n",
    "# - images are casted to float 32 (from uint8) mapped in interval (0,1) and a \"fake\" color channel is added.\n",
    "#   torch uses \"NCHW\"-layout for 2d convolutions. (i.e., a batch of images is represented as a 4 d tensor \n",
    "#   where the first axis (N) is the batch dimension, the second the (color) **C**hannels, followed by a **H**eight\n",
    "#   and a **W**idth axis). As we have grayscale images there is only 1 color channel.  \n",
    "# - targets are mapped to one hot encoding - torch does that for us \n",
    "with torch.no_grad():\n",
    "    train_samples = torch.from_numpy(data_dict[\"train_samples\"].astype(np.float32) / 255.0).reshape(-1, 1, 28, 28)\n",
    "    train_labels = torch.nn.functional.one_hot(torch.from_numpy(data_dict[\"train_labels\"]))\n",
    "    test_samples = torch.from_numpy(data_dict[\"test_samples\"].astype(np.float32) / 255.0).reshape(-1, 1, 28, 28)\n",
    "    test_labels = torch.nn.functional.one_hot(torch.from_numpy(data_dict[\"test_labels\"]))\n",
    "\n",
    "\n",
    "# plot first 25 images in train setp\n",
    "plt.figure(figsize=(25, 1))\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    # drop channel axis for plotting\n",
    "    plt.imshow(train_samples[i, 0], cmap=\"gray\", interpolation=\"none\")\n",
    "    plt.gca().axis(\"off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Specifiying Networks (4 p.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in training a neural network is specifying its architecture. Here we will actually build two networks\n",
    "- classifier_fc: A classifier consisting only of fully connected layers\n",
    "- classifier_conv: A classifier combining, convolutional layers, pooling and fully connected layers\n",
    "\n",
    "In the torch API under torch.nn you can find everything you need. Take a look at the classes \"Linear\", \"ReLU\", \"Softmax\", \"Conv2d\", \"MaxPool2d\" and \"Sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_fc = [\n",
    "    torch.nn.Flatten(), # Flatten image into vector\n",
    "    ## TODO ##\n",
    "    # Hidden Layer 1: 256 neurons, Relu activation\n",
    "    # Hidden Layer 2: 128 neurons, Relu activation\n",
    "    # Outputlayer: 10 neurons (one for each class), softmax activation\n",
    "    ###########\n",
    "]\n",
    "classifier_fc = # TODO \n",
    "\n",
    "\n",
    "layers_conv = [\n",
    "    # Conv Layer 1: 8 filters of 3x3 size, ReLU, Max Pool with size 2x2 and stride 2\n",
    "    # Conv Layer 2: 16 filters of 3x3 size, ReLU, Max Pool with size 2x2 and stride 2\n",
    "    # Flatten\n",
    "    # Fully Connected Layer 1: 64 Neurons, ReLU\n",
    "    # Outputlayer: 10 neurons (one for each class), softmax activation\n",
    "]\n",
    "classifier_conv = # TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we going to use both the classifiers interchangable, pick one here and the rest should work with both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_fc \n",
    "#classifier = classifier_conv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Optimizer and Loss (2 p.)\n",
    "\n",
    "Next we need to specify an optimizer and a loss function. For the optimizer we will use Adam (look at torch.optim) with default parameters and as a loss function we will use the cross-entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = #TODO\n",
    "\n",
    "def cross_entropy_loss(labels: torch.Tensor, predictions: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" Cross entropy Loss:\n",
    "    :param labels: Ground truth class labels (shape; [N, num_classes])\n",
    "    :param predictions: predicted class labels (shape: [N, num_classes])\n",
    "    :return: cross entropy (scalar)\n",
    "    \"\"\"\n",
    "    #TODO#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Data Loader (2 p.) \n",
    "\n",
    "For batch gradient descent we need to shuffle and batch the data, PyTorch provdies some functionality for that in form of the \"DataLoader\" (Look at torch.utils.data). In the simplest form used here, it simply shuffles and batches the data but you can also build more complex preprocessing pipelines. We also need a loader for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "#TODO \n",
    "train_loader = \n",
    "test_loader = \n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Training (2 p.)\n",
    "\n",
    "We now have all ingredients and can implement our train loop and a evaluation procedure. You should get a test set accuracy of > 0.95 with both architectures in 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2  # small number of epochs should be sufficient to get descent performance\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch {:03d}\".format(i + 1))\n",
    "    for batch in train_loader:\n",
    "        #TODO################## \n",
    "        # forward pass        #\n",
    "        # backward pass       #\n",
    "        # update step         #\n",
    "        #######################\n",
    "        train_losses.append(loss.detach().numpy())\n",
    "\n",
    "# Evaluate (we still need batching as evaluating all test points at once would probably melt your Memory)\n",
    "avg_loss = avg_acc = 0\n",
    "for batch in test_loader:\n",
    "    samples, labels = batch\n",
    "    predictions = classifier(samples)\n",
    "    loss = cross_entropy_loss(labels, predictions)\n",
    "    acc = torch.count_nonzero(predictions.argmax(dim=-1) == labels.argmax(dim=-1)) / samples.shape[0]\n",
    "\n",
    "    avg_acc += acc / len(test_loader)\n",
    "    avg_loss += loss / len(test_loader)\n",
    "\n",
    "\n",
    "print(\"Test Set Accuracy: {:.3f}, Test Loss {:.3f}\".format(avg_acc.detach().numpy(), avg_loss.detach().numpy()))\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}