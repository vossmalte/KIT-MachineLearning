{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 5 - ML - Grundverfahren\n",
    "We start by loading our Regression data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_plot = np.load('x_plot.npy')\n",
    "y_plot = np.load('y_plot.npy')\n",
    "\n",
    "# the data noise is 1\n",
    "sigma_y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Bayesian Linear Regression (10 Points)\n",
    "We will start the exercise with Bayesian Linear Regression. First we define some hyperparameters which we fix and do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_features = 5    # number of radial basis functions we want to use\n",
    "lamb = 1e-3       # lambda regularization parameter\n",
    "# the means of the Radial basis functions\n",
    "features_means = np.linspace(np.min(x_plot), np.max(x_plot), n_features)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1) Radial Basis Function Features (5 Points)\n",
    "Remember from lecture 7 the radial basis function feature\n",
    "\\begin{align*}\n",
    "\\phi_i(\\boldsymbol{x})=\\exp\\left(-\\dfrac{||\\boldsymbol{x}-\\boldsymbol{\\mu}_i||^2}{2\\sigma^2}\\right),\n",
    "\\end{align*}\n",
    "for the i. feature with mean $\\boldsymbol{\\mu}_i$ and variance $\\sigma^2$. We will assume to have the same variance for each feature function in the following.<br>\n",
    "We will normalize the features in order to avoid difficulties caused by numerical issues. \n",
    "For that purpose, make sure that you divide the feature vector $\\boldsymbol{\\phi}(\\boldsymbol{x})$ by the sum over all Radial Basis features for each input $\\boldsymbol{x}$. Take out the bias from the normalization. Thus, add the bias to the end of your feature vector <b>after</b> the normalization.<br>\n",
    "Your normalized feature matrix should have the form\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\Phi}=\n",
    "    \\left(\n",
    "    \\begin{array}{cccc}\n",
    "    \\tilde{\\phi}_1(\\boldsymbol{x}_1) & \\tilde{\\phi}_2(\\boldsymbol{x}_1) &...& \\tilde{\\phi}_k(\\boldsymbol{x}_1) &1\\\\\n",
    "    \\vdots &\\vdots &\\vdots &\\vdots &\\vdots\\\\\n",
    "    \\tilde{\\phi}_1(\\boldsymbol{x}_N) & \\tilde{\\phi}_2(\\boldsymbol{x}_N) &...& \\tilde{\\phi}_k(\\boldsymbol{x}_N) & 1\\\\\n",
    "    \\end{array}\n",
    "    \\right),\n",
    "\\end{align*}\n",
    "where the ith row of $\\boldsymbol{\\Phi}$ corresponds to the normalized feature vector $\\boldsymbol{\\phi}_i(\\boldsymbol{x})$ concatenated with the bias 1. <br>\n",
    "<b>Note:</b> The normalization of eacht row in $\\boldsymbol{\\Phi}$ (except for the bias) has to be calculated for each input $\\boldsymbol{x}$ independantly. The normalization constant for ith row is therefore $z_i =\\sum_l^k \\phi_l(\\boldsymbol{x}_i)$, where k is the number of features.<br><br>\n",
    "Implement the following function, which should return the <b>normalized</b> feature matrix stated as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_features(x: np.ndarray, means: np.ndarray, sigma:float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: input parameter (shape: [N, d])\n",
    "    :param means: means of each rbf function (shape: [k, d] (k=num features))\n",
    "    :param sigma: bandwidth parameter. We use the same for all rbfs here\n",
    "    :return : returns the radial basis features including the bias value 1 (shape: [N, k+1])\n",
    "    \"\"\"\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape((-1, 1))\n",
    "\n",
    "    if len(means.shape) == 1:\n",
    "        means = means.reshape((-1, 1))\n",
    "    ############################################################\n",
    "    # TODO Implement the normalized rbf features\n",
    "    features =     #[N, k+1]\n",
    "    ############################################################\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_plot = plt.figure(\"Features\")\n",
    "feat_sigma = 0.6\n",
    "y_featuers = rbf_features(x_plot, features_means, sigma=feat_sigma)\n",
    "plt.plot(x_plot, y_featuers[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2) Posterior Distribution (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will implement the posterior distribution of the parameters for Bayesian Linear Regression as stated in the slides. We will directly make use of the closed-form solutions for the posterior mean and the posterior covariance as stated in the slides. <br>\n",
    "By using the Radial Basis function features (see exercise 1.1)), implement the following function, which should return you the posterior mean and the posterior covariance given the inputs to the function.<br> <br> \n",
    "<b>Note:</b>The data standard deviation $\\sigma_y$ which you also need to implement the equations is fixed to 1 already at the beginning of the notebook and is defined as a global variable. The function therefore does not need $\\sigma_y$ as an argument. You can simply make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_distr(X: np.ndarray, y: np.ndarray, lamb:float, means: np.ndarray, sigma_feat:float):\n",
    "    \"\"\"\n",
    "    :param x: input training data (shape: [N, d])\n",
    "    :param y: output training data (shape: [N, 1])\n",
    "    :param lamb: regularization factor (scalar)\n",
    "    :param means: means of each rbf feature (shape: [k, d])\n",
    "    :param sigma_feat: bandwidth of the features (scalar)\n",
    "    :return : returns the posterior mean (shape: [k+1, 1])\n",
    "                      the posterior covariance (shape: [k+1, k+1]) \n",
    "    \"\"\"\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape((-1, 1))\n",
    "    ############################################################\n",
    "    # TODO Implement the posterior distribution\n",
    "    post_mean =             \n",
    "    post_cov = \n",
    "    ############################################################\n",
    "    return post_mean, post_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3) Predictive Distribution (3Points)\n",
    "In this exercise we will implement the predictive distribution for Bayesian Linear Regression as stated in the slides. We will directly make use of the closed-form solutions for the mean and the variance as stated in the slides. <br>\n",
    "By using the Radial Basis function features (see exercise 1.1)), implement the following function, which should return you the mean and the covariance given the inputs to the function.<br> <br> \n",
    "<b>Note:</b>The data standard deviation $\\sigma_y$ which you also need to implement the equations is fixed to 1 already at the beginning of the notebook and is defined as a global variable. The function therefore does not need $\\sigma_y$ as an argument. You can simply make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_distr(x: np.ndarray, y: np.ndarray, X: np.ndarray, lamb:float, means: np.ndarray, sigma_feat:float):\n",
    "    \"\"\"\"\n",
    "    :param x: input data (shape: [N, d])\n",
    "    :param y: output training data (shape: [N, 1])\n",
    "    :param X: input training data (shape: [N, d])\n",
    "    :param means: means of each rbf feature (shape: [k, d])\n",
    "    :param sigma_feat: bandwidth of the features (scalar)\n",
    "    :return : returns the mean (shape: [N, d])\n",
    "                      the variance (shape: [N])\n",
    "                      of the predictive distribution\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the predictive distribution\n",
    "    mean_x = \n",
    "    var_x = \n",
    "    ############################################################\n",
    "    return mean_x, var_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to perform predictions for weights sampled from the posterior. You don't need to implement anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_lin_regr( weights: np.ndarray, input_features: np.ndarray):\n",
    "    \"\"\"\n",
    "    :param x: input data (shape: [N, d])\n",
    "    :param weights: weights for linear regression (shape: [k+1, 1])\n",
    "    :param input_features: applied features on data to predict on (shape: [N, k+1])\n",
    "    :return : returns the predictions to the inputs\n",
    "    \"\"\"\n",
    "    return input_features @ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the predictive distribution\n",
    "pred_mean, pred_var = predictive_distr(x_plot, y_train, x_train, lamb=lamb, \n",
    "                                       means=features_means, sigma_feat=feat_sigma)\n",
    "\n",
    "# plot the predictive distribution together with the 95%intervall\n",
    "plt.figure('Predictve Distr')\n",
    "plt.plot(x_plot, pred_mean, 'b')\n",
    "plt.fill_between(np.squeeze(x_plot), np.squeeze(pred_mean)-2*np.sqrt(pred_var), \n",
    "                 np.squeeze(pred_mean)+2*np.sqrt(pred_var), alpha=0.2, color='blue')\n",
    "plt.plot(x_train, y_train, 'or')\n",
    "plt.plot(x_plot, y_plot, 'black')\n",
    "\n",
    "# Calculate the posterior distribution for the weights now\n",
    "post_mean, post_cov = posterior_distr(x_train, y_train, lamb=lamb, means=features_means, \n",
    "                                      sigma_feat=feat_sigma)\n",
    "# sample 10 different models and plot them:\n",
    "weights = np.random.multivariate_normal(mean=np.squeeze(post_mean), cov=post_cov, size=(10))\n",
    "example_funcs = np.zeros((weights.shape[0], y_plot.shape[0]))\n",
    "for i in range(weights.shape[0]):\n",
    "    example_funcs[i] = pred_lin_regr(weights[i, :], rbf_features(x_plot, features_means, sigma=feat_sigma))\n",
    "    plt.plot(x_plot, example_funcs[i], 'red', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Gaussian Processes (10 Points)\n",
    "The second part of this exercise will focus on Gaussian Processes. We will apply Gaussian Processes on the same data set as before.<br><br>\n",
    "We fix the hyperparameters of the kernel bandwidth and the inversed lambda ($\\lambda^{-1}$) here first (will get more clear later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_kern = 1\n",
    "inv_lamb = 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1) Kernel Vector (4 Points)\n",
    "Implement the Gaussian kernel presented in lecture 7 as\n",
    "\\begin{align*}\n",
    "    k(\\boldsymbol{x}, \\boldsymbol{x}')= \\exp\\left(-\\dfrac{||\\boldsymbol{x}-\\boldsymbol{x}'||^2}{2\\sigma^2}\\right),\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{x}$ and $\\boldsymbol{x}'$ are inputs  to the kernel.\n",
    "<br>\n",
    "<b>Note:</b> Do not multiply $\\lambda^{-1}$ to the kernel in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_vec(x_prime: np.ndarray, x: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x_prime: input data (shape: [N_2 x d])\n",
    "    :param x: input data (shape: [N_1, d])\n",
    "    :param sigma: bandwidth of the kernel\n",
    "    :return: return kernel vector \n",
    "            (shape: [N_2 x N_1])\n",
    "    \"\"\"\n",
    "    if len(x_prime.shape) == 1:\n",
    "        x_prime = x_prime.reshape((-1, 1))\n",
    "\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape((-1, 1))\n",
    "    ############################################################\n",
    "    # TODO Implement the kernel vector\n",
    "    kernel = np.exp(((x_prime[:,None,:] - x[None,:,:])**2).sum(axis=2)/(-2*sigma**2))\n",
    "    ############################################################\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will calculate the kernal matrix. you do not need to implement anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_mat(X: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X: training data matrix (N_train, d)\n",
    "    :sigma: bandwidth of the kernel(scalar)\n",
    "    :return: the kernel matrix (N_train x N_train)\n",
    "    \"\"\"\n",
    "    return get_kernel_vec(X, X, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2) Predictive Distribution for GPs (6 Points)\n",
    "Implement the predictive distribution for Gaussian Processes as stated in the slides. Make use of the kernel function and the kernel matrix function. <br><br>\n",
    "<b>Note:</b> Do not forget to multiply $\\lambda^{-1}$ to the evaluated kernel.\n",
    "<b>Note:</b> The data variance $\\sigma_y$ is fixed to 1 and was declared at the beginning of the exercise as a global variable. Thus, you can simply use it within the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_distr_gp(x: np.ndarray, y: np.ndarray, X: np.ndarray, sigma_kern:float, inv_lamb:float):\n",
    "    \"\"\"\"\n",
    "    :param x: input data (shape: [N_input, d])\n",
    "    :param y: output training data (shape: [N_train, 1])\n",
    "    :param X: input training data (shape: [N_train, d])\n",
    "    :param sigma_kern: bandwidth of the kernel (scalar)\n",
    "    :param inv_lamb: inverse of lambda (scalar)\n",
    "    :return : returns the mean (shape: [N_input x 1])\n",
    "                      the variance (shape: [N_input])\n",
    "                      of the predictive distribution\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the predictive distribution for GPs\n",
    "    K = get_kernel_mat(X,sigma_kern)\n",
    "    k_x_prime = get_kernel_vec(x,X,sigma_kern)\n",
    "    sigma_y = np.var(y)\n",
    "    K_inverse = np.linalg.solve(K + sigma_y*np.identity(K.shape[0]), np.identity(K.shape[0]))\n",
    "\n",
    "    pred_mean = k_x_prime @ K_inverse @ y\n",
    "\n",
    "    k_prime = get_kernel_vec(x,x,sigma_kern)\n",
    "\n",
    "    pred_var = np.diag(k_prime) + sigma_y - np.diag(k_x_prime @ K_inverse @ k_x_prime.T)\n",
    "\n",
    "    #print(\"mean:\",pred_mean.shape)\n",
    "    #print(\"var:\", pred_var.shape)\n",
    "    ############################################################\n",
    "    return pred_mean, pred_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_kern = 1                   # standard deviation of function noise (given)\n",
    "inv_lamb = 1000             # inverse lambda value -> equivalent to lambda = 1e-3\n",
    "gp_fig = plt.figure()\n",
    "\n",
    "# Let's go through the training data and add on training point to the system in each iteration and let's plot\n",
    "# everything dynamically\n",
    "x_dyn_train = []\n",
    "y_dyn_train = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_dyn_train.append(x_train[i])\n",
    "    y_dyn_train.append(y_train[i])\n",
    "    mean, var = predictive_distr_gp(x_plot, np.array(y_dyn_train), np.array(x_dyn_train), sigma_kern, inv_lamb)\n",
    "    if i % 5 == 0:\n",
    "        plt.figure(gp_fig.number)\n",
    "        gp_fig.clf()\n",
    "        plt.plot(x_plot[:, 0], mean[:, 0])\n",
    "        plt.fill_between(x_plot[:, 0], mean[:, 0] -2*np.sqrt(var), mean[:,0]+2*np.sqrt(var), \n",
    "                         alpha=0.2, edgecolor='r', facecolor='r')\n",
    "        plt.plot(np.array(x_dyn_train), np.array(y_dyn_train), 'rx')\n",
    "        plt.title('i='+ str(i))\n",
    "        plt.pause(0.5)\n",
    "    elif i == x_train.shape[0]-1:\n",
    "        plt.figure(gp_fig.number)\n",
    "        gp_fig.clf()\n",
    "        plt.plot(x_plot[:, 0], mean[:, 0])\n",
    "        plt.fill_between(x_plot[:, 0], mean[:, 0] -2*np.sqrt(var), mean[:,0]+2*np.sqrt(var), \n",
    "                         alpha=0.2, edgecolor='r', facecolor='r')\n",
    "        plt.plot(np.array(x_dyn_train), np.array(y_dyn_train), 'rx')\n",
    "        plt.title('i='+ str(i))\n",
    "        plt.pause(0.5)\n",
    "\n",
    "# now let's see the function approximation with all training data and compare to the ground truth function\n",
    "mean, var = predictive_distr_gp(x_plot, y_train, x_train,sigma_kern, inv_lamb)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_plot[:, 0], mean[:, 0])\n",
    "plt.fill_between(x_plot[:, 0], mean[:, 0] -2*np.sqrt(var), mean[:,0]+2*np.sqrt(var), \n",
    "                 alpha=0.2, edgecolor='r', facecolor='r')\n",
    "plt.plot(np.array(x_train), np.array(y_train), 'rx')\n",
    "plt.plot(x_plot, y_plot, 'g')\n",
    "\n",
    "plt.legend(['mean prediction',  'training points', 'gt-function', '2 $\\sigma$',])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}